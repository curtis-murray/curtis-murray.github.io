[["index.html", "Understanding Patient Experiences in Healthcare through Natural Langauge Processing Chapter 1 Abstract", " Understanding Patient Experiences in Healthcare through Natural Langauge Processing Curtis Murray 2021-06-30 Chapter 1 Abstract To come "],["introduction.html", "Chapter 2 Introduction", " Chapter 2 Introduction TODO: Fill this in "],["papers.html", "Chapter 3 Papers 3.1 Patient Opinion 3.2 Covid Reddit 3.3 Framework Paper", " Chapter 3 Papers 3.1 Patient Opinion In development :) Paper is here for now: knitr::include_graphics(&quot;../../../Apps/Overleaf/Natural Language Processing Reveals Patient Opinions from Online Review Websites/Tex/ios-book-article.pdf&quot;) 3.2 Covid Reddit Link to workshop proceedings: http://workshop-proceedings.icwsm.org/index.php?year=2021 Link to the paper: http://workshop-proceedings.icwsm.org/abstract?id=2021_71 Question: How do we add this to Google Scholar? Accepted by MAISoN 2021 knitr::include_graphics(&quot;../../Publishing/MAISoN 2021/COVID19Positive_Submitted.pdf&quot;) 3.3 Framework Paper 3.3.1 Updates Introduction almost finished Can now link to the app as it’s online 3.3.2 Questions Facebook as a data source? There are a few research papers that use it but I haven’t looked into it in too much detail. Was wondering if @Lewis knew the process for getting data access. Want to talk about why Twitter and Reddit are good and finish with why we use Reddit. Citing data policies and news articles (e.g. “reddit claims 52 million daily users”) when there is no paper Paper is getting long, already over 2.5k words. Should we split into two knitr::include_graphics(&quot;files/template.pdf&quot;) "],["cases-vs-posts.html", "Chapter 4 Cases vs Posts 4.1 Us cases vs COVID-19Positive posts 4.2 Aus cases vs Coronavirusdownunder posts", " Chapter 4 Cases vs Posts Looking at how covid cases correlate with posts to covid subreddits. Looking at two different instances: 4.1 Us cases vs COVID-19Positive posts 4.1.1 TS Plots 4.1.2 Lags Loop through each day. Window of 30 days in past and in future. Look at the CCF 4.1.2.1 Actual points and loess 4.1.2.2 Smoothed with a window of 200 days Same data as before but just using rollmean(lags,k = 200) 4.2 Aus cases vs Coronavirusdownunder posts 4.2.1 TS Plots 4.2.2 Lags 4.2.2.1 Actual points and loess 4.2.2.2 Smoothed with a window of 200 days 4.2.3 Topic Network Vis 4.2.4 Word Frequencies Mean word frequencies per month with loess. "],["topic-modelling.html", "Chapter 5 Topic Modelling 5.1 Latent Dirichlet Allocation 5.2 A network approach to topic modelling", " Chapter 5 Topic Modelling 5.1 Latent Dirichlet Allocation TODO: Explain what this is and how it works 5.2 A network approach to topic modelling Topic modelling can be conducted by performing community detection in a document-word network. The communities of words form topics. TODO: explain how this works 5.2.1 Stochastic block modelling TODO: Explain SBM 5.2.2 Hierarchical stochastic block modelling TODO: Explain hSBM 5.2.3 Constructing a topic structure When conducting topic modelling in the context of a document-word network we get multiple matrices for each layer of depth in the hieararchy. One of these matrices is populated with \\(p(\\text{word} | \\text{topic}, \\text{level})\\). From these matrices we can construct the hierarchical topic structure by noticing that words have non-zero probability for a particular topics if and only if they have a common ancestor at the given level. "],["sampling-problem.html", "Chapter 6 Sampling Problem 6.1 Sampling problem motivation 6.2 Consistency of topics 6.3 Distances between topic strutures 6.4 Working example for computing the topic structure distance 6.5 Topic distances for samples drawn from the same corpus 6.6 Topic distances for samples drawn from the same distrubtion 6.7 Topic distances for different corpora", " Chapter 6 Sampling Problem 6.1 Sampling problem motivation We don’t understand statistical justification for topic modelling. We want topic modeling to be consistent and unbiased. Knowing that topic structures found from topic modelling on documents drawn from the same distribution converge to the topic structure that generated documents would justify the use of topic modelling. 6.2 Consistency of topics If we assume that documents are generated according to a topic structure \\(\\theta\\), and \\(T_s\\) are the topic structures generated from the first \\(s\\) documents generated by the model, then we want; \\[lim_{s \\to \\infty} Pr \\left( \\left&lt;T_s, \\theta \\right&gt; &gt; \\varepsilon \\right) = 0,\\] where \\(\\left&lt;\\cdot,\\cdot \\right&gt;\\) is a distance metric for topic structures. 6.3 Distances between topic strutures To determine the consistency of topics we need a way of computing distances between topic structures. There is no current method for doing so in the literature that incorperates the hierarchical nature of topics found through network topic modelling. Distances between trees with leaf-labelled nodes on a free leafset exists in the literature however it runs in \\(O(N!)\\) time and therefore is infeasible for topic structures which may have vocabularies in the order of tens of thousands of words. (This was attempted but the code never finished running). To be able to compute distances between topic structures we need an efficient method. Below we present a way that runs in \\(O(N^2)\\) time with storage requirements of \\(O(N)\\). This results in the topic distance comparison running in a small enough window of time that allows for pairwise comparisons between multiple topic structures, and hence allows for a deep understanding of the statistical properties of topic modelling. 6.3.1 Simple definition for topic structure distances We can define this distance metric as \\[\\left&lt; T_s,T_m \\right&gt; := \\sum_{i=1}^n\\sum_{j=1}^n |\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*}|,\\]where \\(T^*\\) is the path length matrix and \\(\\hat{T^*}\\) is the restriction of \\(T^*\\) to leaf nodes of \\(T^*\\). Proposition 6.1 \\(\\left&lt; \\cdot, \\cdot \\right&gt;\\) is a distance metric ::: {.proof #metric} We need to show the following: \\(\\left&lt; T_s, T_m \\right&gt; = 0 \\iff T_s = T_m\\) Firstly, \\({T_s}_{ij}^*-{T_m}_{ij}^* = 0 \\iff \\hat{{T_s}_{ij}^*} = \\hat{{T_m}_{ij}^*}\\) for all \\(i\\) and \\(j\\). This implies that \\(\\left&lt; T_s, T_m \\right&gt; = 0 \\iff \\hat{T_s^*} = \\hat{T_m^*}\\). Now \\(T_s = T_m \\implies \\hat{T_s^*} = \\hat{T_m^*}\\) so all that is left is to show the converse implication. It is not difficult to see that the path length matrix restricted to leaf nodes uniquely defines a topic-structure tree (in fact we construct the trees using this information), showing that \\(\\hat{T_s^*} = \\hat{T_m^*} \\implies T_s = T_m\\) and hence \\(\\left&lt; T_s, T_m \\right&gt; = 0 \\iff T_s = T_m\\) as requi#AF97FC. \\(\\left&lt; T_s, T_m \\right&gt; = \\left&lt; T_m, T_s \\right&gt;\\) Clearly \\[ \\sum_{i=1}^n\\sum_{j=1}^n |\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*}| = \\sum_{i=1}^n\\sum_{j=1}^n |\\hat{{T_m}_{ij}^*}-\\hat{{T_s}_{ij}^*}|\\] as \\(|a-b| = |b-a|\\). \\(\\left&lt; T_s, T_k \\right&gt; \\leq \\left&lt; T_s, T_m \\right&gt; + \\left&lt; T_m, T_k \\right&gt;\\) \\[\\begin{align} \\left&lt; T_s, T_k \\right&gt; &amp;= \\sum_{i=1}^n\\sum_{j=1}^n |\\hat{{T_s}_{ij}^*}-\\hat{{T_k}_{ij}^*}| \\\\ &amp;= \\sum_{i=1}^n\\sum_{j=1}^n |\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*}+\\hat{{T_s}_{ij}^*} -\\hat{{T_k}_{ij}^*}| \\\\ &amp;\\leq \\sum_{i=1}^n\\sum_{j=1}^n \\left( |\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*} | + |\\hat{{T_s}_{ij}^*} -\\hat{{T_k}_{ij}^*}| \\right)\\\\ &amp;= \\sum_{i=1}^n\\sum_{j=1}^n |\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*} | + \\sum_{i=1}^n\\sum_{j=1}^n|\\hat{{T_s}_{ij}^*} -\\hat{{T_k}_{ij}^*}| \\\\ &amp;= \\left&lt; T_s, T_m \\right&gt; + \\left&lt; T_m, T_k \\right&gt; \\end{align}\\] 6.3.2 Improved definition for topic structre distances The previous definition makes no distinction between high and low density words. It places the same penalty on high density words being far apart as it does for low density words. An improved model would incorporate this into the distance metric. \\[\\left&lt; T_s,T_m \\right&gt; := \\sum_{i=1}^n\\sum_{j=1}^n p(w_i)p(w_j)|\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*}|,\\] where \\(p(w_i)\\) and \\(p(w_i)\\) are the empirical (or known) densities for words \\(w_i\\) and \\(w_j\\) respectively. If we are comparing topic structures drawn from the same distribution, and \\[\\left&lt; T_s,T_m \\right&gt; := \\sum_{i=1}^n\\sum_{j=1}^n |p({w_s}_i)p({w_s}_j)\\hat{{T_s}_{ij}^*}-p({w_m}_i)p({w_m}_j)\\hat{{T_m}_{ij}^*}|,\\] when the topic structures are drawn from different distributions. EDIT: we can see that the latter is actually pretty bad. I think we want to weight the whole of \\(|\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*}|\\) and not the \\(\\hat{{T_s}_{ij}^*}\\) inside otherwise things go wrong. TODO: check this stuff is still a distance. 6.3.2.1 Defining distances I’ll need to come up with better names for this stuff. 6.3.2.2 Unweighted definition \\[\\left&lt; T_s,T_m \\right&gt; := \\sum_{i=1}^n\\sum_{j=1}^n |\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*}|,\\] 6.3.2.3 Corpora weighted definition \\[\\left&lt; T_s,T_m \\right&gt; := \\sum_{i=1}^n\\sum_{j=1}^n p(w_i)p(w_j)|\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*}|,\\] where \\(p(w_i)\\) and \\(p(w_i)\\) are the empirical densities for words \\(w_i\\) and \\(w_j\\) respectively across all corpora. (This is good when samples are drawn from the same distribution, not when we have multiple different distributions) 6.3.2.4 Ave weighted definition \\[\\left&lt; T_s,T_m \\right&gt; := \\sum_{i=1}^n\\sum_{j=1}^n p(w_i)p(w_j)|\\hat{{T_s}_{ij}^*}-\\hat{{T_m}_{ij}^*}|,\\] where \\(p(w_i)\\) and \\(p(w_i)\\) are the empirical densities for words \\(w_i\\) and \\(w_j\\) respectively found from the two corpora considered, i.e. \\(p(w_i) = \\frac{p(w_i | C_1) + p(w_i | C_2)}{2}\\). 6.3.2.5 Full weighted definition \\[\\left&lt; T_s,T_m \\right&gt; := \\sum_{i=1}^n\\sum_{j=1}^n |p({w_s}_i)p({w_s}_j)\\hat{{T_s}_{ij}^*}-p({w_m}_i)p({w_m}_j)\\hat{{T_m}_{ij}^*}|,\\] 6.4 Working example for computing the topic structure distance 6.4.1 Finding the path length matrix \\(\\hat{T^*}\\) Since we are in a leaf-labelled tree we label to non-leaf nodes with a unique ID. We can represent a tree \\(T\\) by labelling each leaf node by the paths from the root node. Consider the below tree \\(T_1\\): We can represent \\(T_1\\) using the path list below. Table 6.1: node path A A-4-2-1 B B-4-2-1 C C-5-2-1 D D-5-2-1 E E-6-3-1 F F-6-3-1 The path length between two leaf-nodes can be found by finding the first matching labels in their path strings. I.e. for \\(A\\) and \\(C\\) the path strings are “A-4-2-1” and “C-5-2-1”, which first match at Node \\(2\\). There are two uncommon nodes in the string (the nodes themselves, and the immediate parents). This means the common ancestor is two nodes up the tree, and hence the path length between nodes \\(A\\) and \\(C\\) equals \\(4\\). In this way we can construct \\(\\hat{T_1}^*\\), the matrix of path lengths. \\[ \\hat{T_1^*} = \\begin{bmatrix}\\ 0 &amp; 2 &amp; 4 &amp; 4 &amp; 6 &amp; 6\\\\ 2 &amp; 0 &amp; 4 &amp; 4 &amp; 6 &amp; 6\\\\ 4 &amp; 4 &amp; 0 &amp; 2 &amp; 6 &amp; 6\\\\ 4 &amp; 4 &amp; 2 &amp; 0 &amp; 6 &amp; 6\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 0 &amp; 2\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 2 &amp; 0\\\\ \\end{bmatrix} \\] The dimensions of the above matrix are \\(|V|^2\\) where \\(V\\) is the vocabulary of the documents. This would cause memory issues if we were to compute the whole matrix and store it in memory. Instead, we will loop through as the matrix. Also note that in Table the network in the path representation has \\(|V|\\) elements where \\(V\\) is the vocabulary of the documents so this representation is also memory efficient. Consider a second tree \\(T_2\\) that has similar structure, with differences indicated by purple nodes. We could find the path list representation and hence path length matrix \\(\\hat{T_2^*}\\). Table 6.2: node path A A-4-2-1 B B-4-2-1 C C-4-2-1 D D-5-2-1 E E-6-3-1 F F-6-3-1 G G-7-3-1 \\[ \\hat{T_2^*} = \\begin{bmatrix}\\ 0 &amp; 2 &amp; 2 &amp; 4 &amp; 6 &amp; 6 &amp; 6\\\\ 2 &amp; 0 &amp; 2 &amp; 4 &amp; 6 &amp; 6 &amp; 6\\\\ 2 &amp; 2 &amp; 0 &amp; 2 &amp; 6 &amp; 6 &amp; 6\\\\ 4 &amp; 4 &amp; 2 &amp; 0 &amp; 6 &amp; 6 &amp; 6\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 0 &amp; 2 &amp; 2\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 2 &amp; 0 &amp; 2\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 2 &amp; 2 &amp; 0\\\\ \\end{bmatrix} \\] 6.4.2 Finding the topic structre distance \\(\\left&lt;T_1, T_2 \\right&gt;\\) We have a small problem before we can find the distances between the topic structures. Node \\(G\\) was not present in \\(T_1\\). In general, we can overcome this by adding any nodes not present under an entirely new topic hierarchy, so that they have maximum path length from all nodes. In the rest of this problem we will use the Simple definition for topic structure distances. Then the topic structure distance is the sum of the absolute differences of the two path length matrices. Denote the absolute differences by \\(D\\). \\[\\begin{align} D &amp;= \\left| \\begin{bmatrix} 0 &amp; 2 &amp; 4 &amp; 4 &amp; 6 &amp; 6 &amp; 6\\\\ 2 &amp; 0 &amp; 4 &amp; 4 &amp; 6 &amp; 6 &amp; 6\\\\ 4 &amp; 4 &amp; 0 &amp; 2 &amp; 6 &amp; 6 &amp; 6\\\\ 4 &amp; 4 &amp; 2 &amp; 0 &amp; 6 &amp; 6 &amp; 6\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 0 &amp; 2 &amp; 6\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 2 &amp; 0 &amp; 6\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 6 &amp; 6 &amp; 0\\\\ \\end{bmatrix} - \\begin{bmatrix} 0 &amp; 2 &amp; 2 &amp; 4 &amp; 6 &amp; 6 &amp; 6\\\\ 2 &amp; 0 &amp; 2 &amp; 4 &amp; 6 &amp; 6 &amp; 6\\\\ 2 &amp; 2 &amp; 0 &amp; 2 &amp; 6 &amp; 6 &amp; 6\\\\ 4 &amp; 4 &amp; 2 &amp; 0 &amp; 6 &amp; 6 &amp; 6\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 0 &amp; 2 &amp; 2\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 2 &amp; 0 &amp; 2\\\\ 6 &amp; 6 &amp; 6 &amp; 6 &amp; 2 &amp; 2 &amp; 0\\\\ \\end{bmatrix}\\right| \\\\ &amp;= \\begin{bmatrix} 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 2 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 4\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 4\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 4 &amp; 4 &amp; 0\\\\ \\end{bmatrix} \\end{align}\\] Then \\[\\begin{align} \\left&lt; T_1, T_2 \\right&gt; &amp;= \\sum_{i,j} D_{i,j} \\\\ &amp;=24 \\end{align}\\] The code for this is found in Tree Distance Code 6.5 Topic distances for samples drawn from the same corpus NOTICE! This is a work in progress! The following section describes work done using code found in the appendix section Topic distances for samples drawn from the same corpus REPO. Data was scraped from the subreddits; Adelaide, Liverpool, London, Melbourne, Sydney from 2019-01-01 to 2020-11-16. Each of the 44,157 posts across these subreddits is considered a document for the purpose of topic modelling. TODO: Give more info on preprocessing and cleaning. The following links define what I mean by the distances. Unweighted definition Corpora weighted definition Ave weighted definition Full weighted definition This is cool! The corpora weighted distance is probably the most applicable here as we’re drawing samples from the same corpus so we know the densities of the words. Interestingly the full weighted distance goes up, probably a bad formulation as mentioned previously. 6.6 Topic distances for samples drawn from the same distrubtion NOTICE! This is a work in progress! 6.6.1 Idea Find a reasonable topic distribution by running topic modelling on a real corpus. Define a generative process using the hSBM with the topic distribution found in Step 1. Sample according the hSBM generative process BUT - what does this actually mean? In LDA there is a clear generative process for documents as it’s a generative model for documents. hSBM is a generative model that assumes edges are distributed between existing nodes in a network according to a community structure. To generate a new document, we can assume some community structure, and then ADD a document node to the network by randomly assigning it to a community according to the community densities. From this, we can use the hSBM generative process to distribute edges to the word nodes to populate the document. 6.7 Topic distances for different corpora NOTICE! This is a work in progress! Code is at Topic distances for different corpora REPO 6.7.1 Idea Pick a bunch of subreddits. Went with a mix of geographically related cities, sports, and interests. What we hope to see is that there is some similarity in the topic structures for similar subreddits. As an example, we would expect to see small distances between Adelaide, Melbourne, and Sydney. Fit hSBMs to the subreddits individually Find pairwise distances between topic structures Observe using MDS or similar Notes: Tried MDS, t-SNE and UMAP, UMAP looked best. 6.7.2 Pairwise distances between subreddits TODO: - Some subs don’t have a topic structure, str_split won’t work on these but this probably isn’t the main issue. Find out why they don’t have topic structure. - 6.7.2.1 Unweighted The problem here is there are rich topic structures for soccer and conspiracy, making them have a large distance to everything. \\(1/n\\) weighting would be better than no weighting at all. We saw in Topic distances for samples drawn from the same corpus where the vocab size was unchanged; the unweighted strategy is worse anyway. 6.7.2.2 Corpora weighted Structure emerges! Although note that corpora weighted distance isn’t as applicable here as this is looking at the overall word densities for ALL subs, not just the pairs we consider. 6.7.2.3 Ave weighted This is probably what we want as it finds the word densities for the pairs we are considering. 6.7.2.4 Full weighted Again not great. "],["appendix.html", "Chapter 7 Appendix 7.1 Code 7.2 GitHub 7.3 Guides", " Chapter 7 Appendix 7.1 Code 7.1.1 Tree Distance Code import pandas as pd import numpy as np import os import timeit import cProfile import re import time import sys if not os.path.exists(&quot;data/Tree_Distance&quot;): os.system(&quot;mkdir data/Tree_Distance&quot;) # INPUT number of samples sample = int(sys.argv[1]) #os.system(&quot;touch data/Tree_Distance/running_sample_&quot;+str(sample)) print(&quot;Tree distance on sample: &quot; + str(sample)) # Data loading # Tidy topics df = pd.read_csv(&quot;data/Tidy_Topics/tidy_topics_str.csv&quot;) # Vocab Vocab = pd.read_csv(&quot;data/Vocab/Vocab.csv&quot;)[[&#39;word_ID_full&#39;, &#39;freq&#39;]].set_index(&#39;word_ID_full&#39;).T.to_dict(&#39;list&#39;) n_words = len(Vocab) # Preprocessing data # Filter to full data full_data = df.query(&quot;Sample == 0&quot;)[[&quot;word_ID_full&quot;,&quot;topic&quot;]].set_index(&#39;word_ID_full&#39;).T.to_dict(&#39;list&#39;) # Get sample data sample_data = df.query(&quot;Sample == @sample&quot;)[[&quot;word_ID_full&quot;,&quot;topic&quot;]].set_index(&#39;word_ID_full&#39;).T.to_dict(&#39;list&#39;) def total_dist(full_data, sample_data): total_d = 0 max_depth_full = len(list(full_data.items())[1][1][0].split(&quot;-&quot;)) max_depth_sample = len(list(sample_data.items())[1][1][0].split(&quot;-&quot;)) # Nested through upper triangle of adjacency matrix computing weighted # path length on each itteration for i in range(1,n_words+1): for j in range(i+1,n_words+1): total_d += weighted_diff_path_length(i,j, full_data, sample_data, max_depth_full, max_depth_sample) return total_d def weighted_diff_path_length(i,j, full_data, sample_data, max_depth_full, max_depth_sample): # Computed the weighted difference in path lenghts # weighted by p_word(i) and p_word(j) d_full = path_length(i,j, full_data, max_depth_full) d_samp = path_length(i,j, sample_data, max_depth_sample) d = abs((d_full - d_samp)*p_word(i)*p_word(j)) return d def p_word(i): # Returns p(word | full corpus) # Given as the empirical frequency p = Vocab.get(i)[0] return p def path_length(i,j,data, max_depth): # Function to compute path lengths between distinct words topic_i = data.get(i) topic_j = data.get(j) # If either or both words are not part of the data return the max path length (2*depth) if (topic_i is None) | (topic_j is None): return max_depth*2 # If the words are the same then the path length is 0 # Never true as only take upper triangle if i == j: return 0 topic_i = topic_i[0].split(&quot;-&quot;) topic_j = topic_j[0].split(&quot;-&quot;) # import string and look for substrings and stuff # Loop through hierarchy, starting at deepest level # If words are in same topic return distance (starting at 2) # Othewise move up hierarcy and add 2 to path length for depth in range(max_depth): if topic_i[depth] == topic_j[depth]: return (depth+1)*2 d = total_dist(full_data, sample_data) print(&quot;Distance of &quot; + str(d)) pd.DataFrame({&quot;sample&quot;: [sample], &quot;distance&quot;: [d]}).to_csv(&quot;data/Tree_Distance/sample_&quot;+str(sample)+&quot;.csv&quot;, index = False, header=False) 7.2 GitHub 7.2.1 Topic distances for samples drawn from the same corpus REPO https://github.com/curtis-murray/Phoenix-Sampling 7.2.2 Topic distances for different corpora REPO https://github.com/curtis-murray/Topic_distance_different_corpora 7.2.3 Template for DAPMAV Framework https://github.com/curtis-murray/Template_Framework The above github repository contains code and makefiles for the DAPMAV framework. This contains a file ‘subs’ the user to select the subreddits to scrape. Running will do everything :) If you don’t want to rescrape then run If you dont want to run the topic modelling again then you can run TODO: should probably make these makefile names better Note that the default behavior is to scrape the posts AND replies instead of just the posts. I should probably make a script that’s a bit smarter to allow this to be an input argument. 7.3 Guides NOTICE! This is really rough stuff and mostly just so I can remember how to code things that I’m probably going to forget about in a month. 7.3.1 Github For when we fuck up: 7.3.2 Phoenix For making slow code fast! Or, more accurately; for doing lots of things at once :) 7.3.2.1 VPN If you are working from a network outside of uni, you will need to VPN in to access Phoenix. Linux users can install the vpn by following the guide put up by Adelaide Uni: https://www.adelaide.edu.au/technology/system/files/media/documents/2020-07/diy-linux-vpn.pdf. To use the vpn through terminal you can do it as follows /opt/cisco/anyconnect/bin/vpn You can then type in the following command connect vpn.adelaide.edu.au You will be prompted to input your Adelaide uni username and password. Your second password will be the authenticator code from Duo Mobile. If you want to check to see if it’s working you can open a new terminal and enter the command nmcli con show --active If you see something that looks like cscotun0 then it should be good, and you’re ready to ssh in. 7.3.3 SSH in to Phoenix ssh a1670295@phoenix-login1.adelaide.edu.au source /hpcfs/users/$USER/local/virtualenvs/bin/activate 7.3.4 Running code 7.3.4.1 SLURM Setup To run code on phoenix you need to use the resource manager SLURM I really hope they did this intentionally. To run a job with SLURM you need to set it up right. 7.3.4.2 Batching a job To run this job, we can call it as follows If this worked, you’ll be able to see it in the queue. 7.3.4.3 Batching multiple jobs I’m running lots of simulations that have different parameters. To do this, I call them from an external loop. Here my SLURM script is called tree_dist.sh We can save this in a file ‘run_tree_dist.sh’ and make it executable. 7.3.4.4 Dependencies in SLURM Sometimes we need to run a file after we’ve run something else, i.e. we have some dependency in our workload. We can deal with this in SLURM by being lazy. Here is a link that can explain it better and in more detail than I can. "]]
